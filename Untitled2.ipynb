{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D_Rz0g_zEgyz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15c951a5-8c3c-40d0-ccaf-804d4dabda3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.33.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.31.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers) (0.5.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers) (11.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.1 (from gradio)\n",
            "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.31.0-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m111.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, groovy, ffmpy, aiofiles, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, safehttpx, nvidia-cusolver-cu12, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.31.0 gradio-client-1.10.1 groovy-0.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.11 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install diffusers transformers torch gradio\n",
        "!pip install -q accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import gradio as gr\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "import psutil\n",
        "from PIL import Image\n",
        "\n",
        "# Detect if running in Colab\n",
        "IN_COLAB = 'COLAB_GPU' in os.environ\n",
        "\n",
        "# Custom cyan loss implementation - FIXED FOR LATENT SPACE\n",
        "def cyan_loss(step: int, timestep: torch.FloatTensor, latents: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Custom cyan effect callback that works properly in SD's latent space.\"\"\"\n",
        "    try:\n",
        "        # Input validation\n",
        "        if not isinstance(step, int) or step < 0:\n",
        "            return latents\n",
        "\n",
        "        if not torch.is_tensor(latents):\n",
        "            return latents\n",
        "\n",
        "        if not torch.is_tensor(timestep):\n",
        "            return latents\n",
        "\n",
        "        # Only apply effect in later steps for stability\n",
        "        if step < 30:\n",
        "            return latents\n",
        "\n",
        "        device = latents.device\n",
        "\n",
        "        # Ensure we have 4-channel latent space\n",
        "        if latents.shape[1] != 4:\n",
        "            return latents\n",
        "\n",
        "        # PROPER LATENT SPACE MANIPULATION FOR CYAN BIAS\n",
        "        # Instead of treating channels as RGB, we apply a subtle transformation\n",
        "        # that encourages cyan-like features in the final decoded image\n",
        "\n",
        "        # Create a cyan-encouraging transformation matrix\n",
        "        # This is based on how SD's VAE decoder interprets latent channels\n",
        "        cyan_transform = torch.tensor([\n",
        "            [0.8, 0.1, 0.1, 0.0],   # Channel 0: reduce slightly\n",
        "            [0.1, 1.2, 0.1, 0.0],   # Channel 1: boost (often correlates with green-blue)\n",
        "            [0.1, 0.1, 1.3, 0.0],   # Channel 2: boost more (often correlates with blue)\n",
        "            [0.0, 0.0, 0.0, 1.0]    # Channel 3: keep unchanged (structure)\n",
        "        ], device=device, dtype=latents.dtype)\n",
        "\n",
        "        # Apply transformation\n",
        "        batch_size, channels, height, width = latents.shape\n",
        "        latents_flat = latents.view(batch_size, channels, -1)  # Flatten spatial dims\n",
        "\n",
        "        # Apply the transformation\n",
        "        transformed_flat = torch.matmul(cyan_transform, latents_flat)\n",
        "        cyan_latents = transformed_flat.view(batch_size, channels, height, width)\n",
        "\n",
        "        # Progressive application with moderate strength\n",
        "        progress = min((step - 30) / 15.0, 1.0)  # Ramp up over 15 steps\n",
        "        strength = 0.25 * progress  # Much gentler effect\n",
        "\n",
        "        # Blend original and transformed latents\n",
        "        modified = latents * (1 - strength) + cyan_latents * strength\n",
        "\n",
        "        # Ensure values stay in reasonable range for stability\n",
        "        modified = torch.clamp(modified, -4.0, 4.0)\n",
        "\n",
        "        if step % 10 == 0:  # Less frequent logging\n",
        "            print(f\"Step {step}: Applying cyan transformation with strength {strength:.3f}\")\n",
        "\n",
        "        return modified\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in cyan_loss: {str(e)}\")\n",
        "        return latents\n",
        "\n",
        "# Style configurations with enhanced names\n",
        "STYLES = {\n",
        "    \"✨ Dreamy Portrait\": {\n",
        "        \"embedding_path\": \"buhu.bin\",\n",
        "        \"seed\": 42,\n",
        "        \"token\": \"<buhu>\"\n",
        "    },\n",
        "    \"🌆 Neon Cyberpunk\": {\n",
        "        \"embedding_path\": \"cyberpunk.bin\",\n",
        "        \"seed\": 123,\n",
        "        \"token\": \"<cyberpunk>\"\n",
        "    },\n",
        "    \"🎋 Asian Anime\": {\n",
        "        \"embedding_path\": \"hanfu_animestyle.bin\",\n",
        "        \"seed\": 456,\n",
        "        \"token\": \"<hanfu>\"\n",
        "    },\n",
        "    \"⭐ Pokemon Style\": {\n",
        "        \"embedding_path\": \"pokemon.bin\",\n",
        "        \"seed\": 321,\n",
        "        \"token\": \"<pokemon>\"\n",
        "    },\n",
        "    \"🏛️ Solomon Temple\": {\n",
        "        \"embedding_path\": \"solomon temple.bin\",\n",
        "        \"seed\": 888,\n",
        "        \"token\": \"<solomon>\"\n",
        "    }\n",
        "}\n",
        "\n",
        "def generate_image(prompt, style_name, use_cyan_loss=False, seed=None):\n",
        "    \"\"\"Generate an image with the specified style\"\"\"\n",
        "    print(\"\\n🎨 Starting image generation...\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Style: {style_name}\")\n",
        "    print(f\"Use cyan loss: {use_cyan_loss}\")\n",
        "    print(f\"Seed: {seed}\")\n",
        "\n",
        "    # Initial memory optimization\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    optimize_memory(device)\n",
        "\n",
        "    if not prompt or prompt.strip() == \"\":\n",
        "        raise ValueError(\"Prompt cannot be empty\")\n",
        "\n",
        "    if seed is None:\n",
        "        seed = STYLES[style_name][\"seed\"]\n",
        "\n",
        "    # Set device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        # Initialize the pipeline with optimized settings        print(\"\\n📦 Loading Stable Diffusion model...\")\n",
        "        model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "            safety_checker=None,\n",
        "            requires_safety_checker=False,\n",
        "            cache_dir=\"./models\"\n",
        "        ).to(device)\n",
        "\n",
        "        # Enhanced memory optimizations with safer configuration\n",
        "        pipeline.enable_attention_slicing(slice_size=\"max\")\n",
        "        if device == \"cuda\":\n",
        "            pipeline.enable_vae_slicing()\n",
        "            pipeline.enable_model_cpu_offload()  # Use model CPU offload instead of sequential\n",
        "        print(\"✓ Advanced memory optimizations enabled\")\n",
        "\n",
        "        # Load style embedding\n",
        "        style_info = STYLES[style_name]\n",
        "        embedding_path = Path(style_info['embedding_path'])\n",
        "        token = style_info['token']\n",
        "\n",
        "        print(f\"\\n🔄 Loading style: {style_name}\")\n",
        "        print(f\"Path: {embedding_path.absolute()}\")\n",
        "\n",
        "        if not embedding_path.exists():\n",
        "            raise FileNotFoundError(f\"Embedding file not found: {embedding_path}\")\n",
        "\n",
        "        # Load embedding\n",
        "        learned_embeds = torch.load(embedding_path, map_location=device)\n",
        "        if not learned_embeds or not isinstance(learned_embeds, dict):\n",
        "            raise ValueError(\"Invalid embedding file format\")\n",
        "\n",
        "        # Get embedding vector\n",
        "        key = list(learned_embeds.keys())[0]\n",
        "        embedding_vector = learned_embeds[key]\n",
        "        if not isinstance(embedding_vector, torch.Tensor):\n",
        "            raise ValueError(\"Embedding data is not a tensor\")\n",
        "\n",
        "        # Add token to tokenizer\n",
        "        special_tokens_dict = {'additional_special_tokens': [token]}\n",
        "        num_added = pipeline.tokenizer.add_special_tokens(special_tokens_dict)\n",
        "        if num_added > 0:\n",
        "            pipeline.text_encoder.resize_token_embeddings(len(pipeline.tokenizer))\n",
        "\n",
        "        # Get token ID and verify\n",
        "        token_id = pipeline.tokenizer.convert_tokens_to_ids(token)\n",
        "        if token_id == pipeline.tokenizer.unk_token_id:\n",
        "            raise ValueError(f\"Token {token} not properly added to tokenizer\")\n",
        "\n",
        "        # Handle embedding dimensions\n",
        "        required_dim = pipeline.text_encoder.get_input_embeddings().weight.data[0].shape[0]\n",
        "        if embedding_vector.shape[0] != required_dim:\n",
        "            print(f\"⚠️ Reshaping embedding from {embedding_vector.shape[0]} to {required_dim}\")\n",
        "            if embedding_vector.shape[0] > required_dim:\n",
        "                embedding_vector = embedding_vector[:required_dim]\n",
        "            else:\n",
        "                padding = torch.zeros(required_dim - embedding_vector.shape[0], device=device)\n",
        "                embedding_vector = torch.cat([embedding_vector, padding])\n",
        "\n",
        "        # Store embedding\n",
        "        pipeline.text_encoder.get_input_embeddings().weight.data[token_id] = embedding_vector\n",
        "\n",
        "        # Verify token works\n",
        "        test_prompt = f\"test {token} prompt\"\n",
        "        input_ids = pipeline.tokenizer.encode(test_prompt, return_tensors=\"pt\")\n",
        "        if token_id not in input_ids[0]:\n",
        "            raise ValueError(f\"Token verification failed - {token} not found in test prompt\")\n",
        "\n",
        "        print(f\"✓ Style {style_name} loaded successfully\")\n",
        "\n",
        "        # Prepare prompt with style\n",
        "        styled_prompt = f\"{token} {prompt}\"\n",
        "        print(f\"📝 Using prompt: {styled_prompt}\")\n",
        "\n",
        "        # Generate image with optimized parameters\n",
        "        print(\"🎨 Generating image...\")\n",
        "        generator = torch.Generator(device=device).manual_seed(seed)\n",
        "\n",
        "        generation_params = {\n",
        "            \"prompt\": styled_prompt,\n",
        "            \"generator\": generator,\n",
        "            \"num_inference_steps\": 45,  # Slightly reduced for better speed/quality balance\n",
        "            \"guidance_scale\": 8.5,      # Increased for better prompt adherence\n",
        "            \"height\": 512,              # Ensure consistent dimensions\n",
        "            \"width\": 512\n",
        "        }\n",
        "\n",
        "        if use_cyan_loss:\n",
        "            def callback_wrapper(pipe, step_index, timestep, callback_kwargs):\n",
        "                latents = callback_kwargs[\"latents\"]\n",
        "                # Clear CUDA cache periodically to prevent memory buildup\n",
        "                if step_index % 10 == 0 and torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "                return {\"latents\": cyan_loss(step_index, timestep, latents)}\n",
        "\n",
        "            generation_params[\"callback_on_step_end\"] = callback_wrapper\n",
        "\n",
        "        # Generate with optimized memory handling\n",
        "        with torch.inference_mode():\n",
        "            image = pipeline(**generation_params).images[0]\n",
        "\n",
        "        # Clean up CUDA cache after generation\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        print(\"✨ Image generated successfully\")\n",
        "        return image\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"\"\"\n",
        "❌ Error during image generation:\n",
        "Type: {type(e).__name__}\n",
        "Message: {str(e)}\n",
        "Style: {style_name}\n",
        "Device: {device}\n",
        "Memory: {torch.cuda.memory_summary() if torch.cuda.is_available() else 'No GPU'}\n",
        "\"\"\"\n",
        "        print(error_msg)\n",
        "        raise Exception(f\"Image generation failed: {str(e)}\") from e\n",
        "\n",
        "# Memory optimization function\n",
        "def optimize_memory(device, previous_image=None):\n",
        "    \"\"\"Clean up memory and caches\"\"\"\n",
        "    try:\n",
        "        if previous_image and isinstance(previous_image, Image.Image):\n",
        "            previous_image.close()\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            # GPU cleanup\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.ipc_collect()\n",
        "\n",
        "        # CPU cleanup\n",
        "        gc.collect()\n",
        "\n",
        "        # Log memory status\n",
        "        process = psutil.Process()\n",
        "        print(f\"Memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB\")\n",
        "        if device == \"cuda\":\n",
        "            print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "    except Exception as e:\n",
        "        print(f\"Memory optimization error: {str(e)}\")\n",
        "\n",
        "# Gradio interface\n",
        "def create_interface():\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# Artistic Style Transfer with Cyan Loss\")\n",
        "        gr.Markdown(\"*Generate AI art with Stable Diffusion and optional cyan color effect*\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                prompt = gr.Textbox(label=\"Enter your prompt\", placeholder=\"A serene landscape...\")\n",
        "                style = gr.Dropdown(\n",
        "                    choices=list(STYLES.keys()),\n",
        "                    value=\"✨ Dreamy Portrait\",\n",
        "                    label=\"Select Style\"\n",
        "                )\n",
        "                use_cyan = gr.Checkbox(label=\"Apply Cyan Loss Effect\", value=False)\n",
        "                seed = gr.Number(label=\"Seed (optional)\", value=None)\n",
        "                btn = gr.Button(\"Generate\")\n",
        "\n",
        "            with gr.Column():\n",
        "                output = gr.Image(label=\"Generated Image\", type=\"pil\")\n",
        "                status = gr.Markdown(\"System Status\")\n",
        "                error_output = gr.Markdown(\"Ready to generate images...\")\n",
        "\n",
        "                info = gr.Markdown(\"\"\"\n",
        "## Quick Instructions:\n",
        "1. Enter a detailed prompt describing your desired image\n",
        "2. Select one of the 5 artistic styles\n",
        "3. Toggle \"Apply Cyan Loss Effect\" for a blue-green tinted aesthetic\n",
        "4. Optionally set a seed number for reproducible results\n",
        "5. Click \"Generate\" and wait for your image\n",
        "                \"\"\")\n",
        "\n",
        "        def wrapped_generate(*args):\n",
        "            prompt = args[0] if args else None\n",
        "            style_name = args[1] if len(args) > 1 else None\n",
        "\n",
        "            # Validate inputs\n",
        "            if not prompt or prompt.strip() == \"\":\n",
        "                error_msg = \"⚠️ Please enter a prompt. The prompt cannot be empty.\"\n",
        "                print(error_msg)\n",
        "                status.value = \"❌ Generation failed - No prompt provided\"\n",
        "                error_output.value = error_msg\n",
        "                return None\n",
        "\n",
        "            if not style_name or style_name not in STYLES:\n",
        "                error_msg = \"⚠️ Please select a valid style.\"\n",
        "                print(error_msg)\n",
        "                status.value = \"❌ Generation failed - Invalid style\"\n",
        "                error_output.value = error_msg\n",
        "                return None\n",
        "\n",
        "            status.value = \"⏳ Generating image... Please wait...\"\n",
        "            error_output.value = \"\"\n",
        "\n",
        "            try:\n",
        "                print(\"\\nStarting generation...\")\n",
        "                print(f\"Input args: {args}\")\n",
        "\n",
        "                result = generate_image(*args)\n",
        "\n",
        "                if result is None:\n",
        "                    raise Exception(\"Generation returned no image\")\n",
        "\n",
        "                print(\"Generation completed, updating UI...\")\n",
        "                status.value = \"✅ Image generated successfully!\"\n",
        "                error_output.value = \"\"\n",
        "                return result\n",
        "\n",
        "            except Exception as e:\n",
        "                error_msg = f\"\"\"\n",
        "🚨 Error Details:\n",
        "Type: {type(e).__name__}\n",
        "Message: {str(e)}\n",
        "\n",
        "Current directory: {Path.cwd()}\n",
        "Available files in sd-concepts-library:\n",
        "{list(Path('sd-concepts-library').glob('*')) if Path('sd-concepts-library').exists() else 'Directory not found'}\n",
        "\n",
        "💡 Troubleshooting:\n",
        "1. Check if style files exist in the correct location\n",
        "2. Make sure you have enough GPU memory\n",
        "3. Try a simpler prompt\n",
        "\"\"\"\n",
        "                print(\"\\nError occurred:\")\n",
        "                print(error_msg)\n",
        "                status.value = \"❌ Generation failed\"\n",
        "                error_output.value = error_msg\n",
        "                return None\n",
        "\n",
        "        btn.click(\n",
        "            fn=wrapped_generate,\n",
        "            inputs=[prompt, style, use_cyan, seed],\n",
        "            outputs=[output]\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        print(\"Starting Gradio interface...\")\n",
        "        print(\"\\nGPU Status:\")\n",
        "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"GPU Device: {torch.cuda.get_device_name()}\")\n",
        "            print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "        app = create_interface()\n",
        "        # Launch with simpler configuration\n",
        "        app.launch(share=True)  # This will create both local and public URLs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error starting the app: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "BjY5oGD75O2o",
        "outputId": "5835d564-324b-4833-c1b5-a1e802a9d89c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Gradio interface...\n",
            "\n",
            "GPU Status:\n",
            "CUDA available: True\n",
            "GPU Device: Tesla T4\n",
            "GPU Memory: 15.83 GB\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e2a466764b1db990b7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e2a466764b1db990b7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "3tnpM9a-70r_",
        "outputId": "b4ea5d1b-ea0e-4dfb-9061-af86df55abeb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Gradio interface...\n",
            "\n",
            "GPU Status:\n",
            "CUDA available: True\n",
            "GPU Device: Tesla T4\n",
            "GPU Memory: 15.83 GB\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://cdd5e5e3e445d035f9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cdd5e5e3e445d035f9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hnJeW4fcBhOQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}